Consider a system that can recognise a person based just on the phrase "a man in a red hoodie with a backpack." CLIP fails with fine-grained, instance-level reasoning that is necessary for text-based person retrieval, even if it offers a strong foundation for visual-text alignment. Inspired by IRRA, we expand on CLIP by introducing an Implicit Relation Reasoning module that learns cross-modal token connections through masked language modelling. In order to align image-text embeddings, we additionally use Similarity Distribution Matching (SDM), which minimises the KL divergence between their similarity distributions. Without the need for extra oversight or part annotations, our system provides a comprehensive end-to-end pipeline that enables users to add videos, text searches, and image targets, as well as retrieve and monitor results after recording. We show that our system is effective by achieving notable improvements on CUHK-PEDES.

FYP Thesis == FYP report (1).pdf https://github.com/huzaifarashid3/person-reid/blob/main/FYP%20REPORT%20(1).pdf
